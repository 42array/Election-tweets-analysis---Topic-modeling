The goal of this project was to find out if using Topic Modeling information helps improve sentiment classification accuracy in the Kaggle 2020 US Election Tweets Analysis dataset. 

This dataset contains tweets covering the 2020 US elections and was divided into two separate groups: Biden-related tweets and Trump-related tweets. Each set of tweets was independently analyzed to test whether topic modeling has an impact on classification performance. 
The first step involved preparing the two datasets for LDA topic modeling and sentiment classification. Note that the following steps were applied to each dataset separately. The rows with duplicate tweets or empty tweets were removed. Stopwords, punctuation, extra spaces, and URLs were removed from the tweet texts. I decided to allow # and @ to remain in the datasets because I felt that the hashtags or tagging used in the tweets might reveal information about the topics being discussed or the sentiment of the tweets. I also made all tweet text lowercase. 
Because the tweets were previously scraped from Twitter, they were not naturally categorized into sentiment classes. I used the code provided with the assignment to create a label for each tweet, classifying them into one of the following sentiment classes: positive, negative, neutral. While this step was not relevant to the topic modeling task, it would be important for the sentiment classification later on. The last step in data preparation involved tokenizing the cleaned tweets by first applying count vectorization to get the frequency vectors and then applying TF-IDF vectorization to get the TF-IDF vector representation of each tweet. This step formatted the tweets for input to both the LDA model and the logistic regression models. 
LDA topic modeling was then used to determine the topic distributions for each tweet and the word distributions for each topic. I used a visualization library called pyLDAvis to visualize the most important topics across each dataset (biden and trump). I tuned the number of topics by observing whether the resulting topics showed any overlap or whether they were well separated. 
Lastly, I trained two versions of a logistic regression model for each dataset. The first version was a vanilla logistic regression model which learned to classify tweet sentiment given only the TF-IDF vector of each tweet. The second version used a feature vector which combined the TF-IDF vector with the topic distribution of each tweet in order to classify the tweetâ€™s sentiment. The hyperparameters of each logistic regression model were tuned using 10-fold cross validation. I compared the AUC-ROC scores for both model versions for both datasets to see if topic modeling improved sentiment classification performance.
